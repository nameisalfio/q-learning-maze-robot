{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1c6792c",
   "metadata": {},
   "source": [
    "## Libraries and Variables setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "746b4002",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "# Basic libraries\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import random\n",
    "from collections import deque\n",
    "\n",
    "# PyTorch libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Normal\n",
    "\n",
    "# System libraries\n",
    "from lib.data.dataplot import *\n",
    "from lib.system.cart import *\n",
    "from lib.system.controllers import *\n",
    "from lib.system.trajectory import *\n",
    "from lib.system.polar import *\n",
    "from lib.dds.dds import *\n",
    "from lib.utils.time import *\n",
    "\n",
    "# Check available compute devices\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")  # Apple Silicon\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Initialize DDS communication\n",
    "dds = DDS()\n",
    "dds.start()\n",
    "\n",
    "class PolarWheelSpeedControl:\n",
    "    \n",
    "    def __init__(self, _wheelbase, _kp, _ki, _kd, _sat):\n",
    "        self.wheelbase = _wheelbase\n",
    "        self.left_pid = PID_Controller(_kp, _ki, _kd, _sat)\n",
    "        self.right_pid = PID_Controller(_kp, _ki, _kd, _sat)\n",
    "        \n",
    "    def evaluate(self, delta_t, target_linear, target_angular, current_left, current_right):\n",
    "        self.target_left = target_linear - (target_angular * self.wheelbase) / 2.0\n",
    "        self.target_right = target_linear + (target_angular * self.wheelbase) / 2.0\n",
    "        out_left = self.left_pid.evaluate(delta_t, self.target_left - current_left)\n",
    "        out_right = self.right_pid.evaluate(delta_t, self.target_right - current_right)\n",
    "        return (out_left, out_right)\n",
    "\n",
    "# Initialize robot controllers\n",
    "wheel_speed_control = PolarWheelSpeedControl(0.5,    # wheelbase\n",
    "                                         1.0, 5.0, 0.0, None)  # PID parameters\n",
    "\n",
    "# Initialize robot model - physical properties\n",
    "cart2d = TwoWheelsCart2DEncodersOdometry(1.0,        # 1 kg\n",
    "                                     0.3,        # 30cm radius\n",
    "                                     0.9, 0.8,   # linear and angular friction\n",
    "                                     0.04, 0.04, # traction wheels radius 4cm\n",
    "                                     0.4,        # traction wheels timebase 40cm\n",
    "                                     0.03, 0.03, # encoder wheels radius 3cm\n",
    "                                     0.5,        # encoder wheels timebase 50cm\n",
    "                                     4096)       # resolution 4096 tick/rev\n",
    "\n",
    "# Position controller - parameters for linear and angular movement\n",
    "polar_position = Polar2DController(8, 10.0,        # kp = 8, vmax = 10 m/s\n",
    "                                 8, 6.0)         # kp = 8, wmax = 6 rad/s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a79d7ff",
   "metadata": {},
   "source": [
    "## Implementation of Actor-Critic RL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c2d27a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actor Network: Generates target positions (x,y)\n",
    "class ActorNetwork(nn.Module):\n",
    "    def __init__(self, state_size, action_size, hidden_size=64):\n",
    "        super(ActorNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, action_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = torch.tanh(self.fc3(x))  # Output in range [-1, 1]\n",
    "        return x\n",
    "\n",
    "# Critic Network: Evaluates state values\n",
    "class CriticNetwork(nn.Module):\n",
    "    def __init__(self, state_size, hidden_size=64):\n",
    "        super(CriticNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Actor-Critic Agent Implementation\n",
    "class A2CAgent:\n",
    "    def __init__(self, state_size, action_size=2):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        \n",
    "        # Algorithm parameters\n",
    "        self.gamma = 0.99       # Discount factor\n",
    "        self.actor_lr = 0.0001  # Actor learning rate\n",
    "        self.critic_lr = 0.001  # Critic learning rate\n",
    "        self.action_std_dev = 0.5  # Exploration noise standard deviation\n",
    "        self.min_action_std_dev = 0.1  # Minimum exploration\n",
    "        self.std_dev_decay = 0.9995  # Decay rate for exploration\n",
    "        \n",
    "        # Experience replay memory\n",
    "        self.memory = deque(maxlen=10000)\n",
    "        self.batch_size = 32\n",
    "        \n",
    "        # Initialize Actor and Critic networks\n",
    "        self.actor = ActorNetwork(state_size, action_size).to(device)\n",
    "        self.critic = CriticNetwork(state_size).to(device)\n",
    "        \n",
    "        # Optimizers\n",
    "        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=self.actor_lr)\n",
    "        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=self.critic_lr)\n",
    "        \n",
    "        # Training metrics\n",
    "        self.episode_rewards = []\n",
    "    \n",
    "    def select_action(self, state):\n",
    "        # Convert state to PyTorch tensor\n",
    "        state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "        \n",
    "        # Get action from actor network (no gradient tracking)\n",
    "        with torch.no_grad():\n",
    "            action_mean = self.actor(state_tensor).cpu().numpy()[0]\n",
    "        \n",
    "        # Add exploration noise\n",
    "        noise = np.random.normal(0, self.action_std_dev, size=self.action_size)\n",
    "        action = action_mean + noise\n",
    "        \n",
    "        # Clip actions to valid range\n",
    "        action = np.clip(action, -1, 1)\n",
    "        \n",
    "        return action, action_mean\n",
    "    \n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        # Store experience in replay memory\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    def decay_exploration(self):\n",
    "        # Reduce exploration over time\n",
    "        self.action_std_dev = max(self.min_action_std_dev, \n",
    "                                 self.action_std_dev * self.std_dev_decay)\n",
    "    \n",
    "    def train(self):\n",
    "        # Skip if not enough samples\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "        \n",
    "        # Sample batch from memory\n",
    "        minibatch = random.sample(self.memory, self.batch_size)\n",
    "        \n",
    "        # Convert to PyTorch tensors\n",
    "        states = torch.FloatTensor([experience[0] for experience in minibatch]).to(device)\n",
    "        actions = torch.FloatTensor([experience[1] for experience in minibatch]).to(device)\n",
    "        rewards = torch.FloatTensor([experience[2] for experience in minibatch]).to(device)\n",
    "        next_states = torch.FloatTensor([experience[3] for experience in minibatch]).to(device)\n",
    "        dones = torch.FloatTensor([experience[4] for experience in minibatch]).to(device)\n",
    "        \n",
    "        # Get current value estimates\n",
    "        current_values = self.critic(states).squeeze()\n",
    "        \n",
    "        # Compute target values (TD targets)\n",
    "        with torch.no_grad():\n",
    "            next_values = self.critic(next_states).squeeze()\n",
    "            target_values = rewards + self.gamma * next_values * (1 - dones)\n",
    "        \n",
    "        # Compute critic loss (MSE)\n",
    "        critic_loss = F.mse_loss(current_values, target_values)\n",
    "        \n",
    "        # Update critic network\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        self.critic_optimizer.step()\n",
    "        \n",
    "        # Compute advantages for actor update\n",
    "        advantage = (target_values - current_values).detach()\n",
    "        \n",
    "        # Update actor network - policy gradient approach\n",
    "        predicted_actions = self.actor(states)\n",
    "        actor_loss = -torch.mean(\n",
    "            torch.sum(\n",
    "                -torch.square(actions - predicted_actions) * advantage.unsqueeze(1),\n",
    "                dim=1\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "        \n",
    "        # Reduce exploration\n",
    "        self.decay_exploration()\n",
    "    \n",
    "    def save_models(self, actor_path='actor_model.pth', critic_path='critic_model.pth'):\n",
    "        # Save model weights\n",
    "\n",
    "        actor_path = os.path.join(\"models\", actor_path)\n",
    "        critic_path = os.path.join(\"models\", critic_path)\n",
    "\n",
    "        # Ensure directory exists\n",
    "        os.makedirs(os.path.dirname(actor_path), exist_ok=True)       \n",
    "        os.makedirs(os.path.dirname(critic_path), exist_ok=True) \n",
    "\n",
    "        torch.save(self.actor.state_dict(), actor_path)\n",
    "        torch.save(self.critic.state_dict(), critic_path)\n",
    "        print(f\"Models saved to {actor_path} and {critic_path}\")\n",
    "    \n",
    "    def load_models(self, actor_path='actor_model.pth', critic_path='critic_model.pth'):\n",
    "        # Load model weights\n",
    "        self.actor.load_state_dict(torch.load(actor_path))\n",
    "        self.critic.load_state_dict(torch.load(critic_path))\n",
    "        self.actor.eval()\n",
    "        self.critic.eval()\n",
    "        print(f\"Models loaded from {actor_path} and {critic_path}\")\n",
    "\n",
    "# Robot Environment wrapper for RL\n",
    "class RobotEnvironment:\n",
    "    def __init__(self, dds, cart2d, polar_position):\n",
    "        # References to existing objects\n",
    "        self.dds = dds\n",
    "        self.cart2d = cart2d\n",
    "        self.polar_position = polar_position\n",
    "        \n",
    "        # Goal position\n",
    "        self.goal_x = 0.0\n",
    "        self.goal_y = 1.5\n",
    "        \n",
    "        # Environment boundaries\n",
    "        self.env_min_x = -5.0\n",
    "        self.env_max_x = 5.0\n",
    "        self.env_min_y = -5.0\n",
    "        self.env_max_y = 5.0\n",
    "        \n",
    "        # Current target position\n",
    "        self.target_x = 0.0\n",
    "        self.target_y = 0.0\n",
    "        \n",
    "        # Previous distance for reward calculation\n",
    "        self.prev_distance = None\n",
    "        \n",
    "        # Timer for movement control\n",
    "        self.t = Time()\n",
    "    \n",
    "    def get_state(self):\n",
    "        # Get current robot pose\n",
    "        pose = self.cart2d.get_pose()\n",
    "        x, y, theta = pose[0], pose[1], pose[2]\n",
    "        \n",
    "        # Calculate distance to final goal\n",
    "        distance = np.sqrt((x - self.goal_x)**2 + (y - self.goal_y)**2)\n",
    "        \n",
    "        # Calculate angle to goal relative to robot orientation\n",
    "        angle_to_goal = np.arctan2(self.goal_y - y, self.goal_x - x) - theta\n",
    "        angle_to_goal = np.arctan2(np.sin(angle_to_goal), np.cos(angle_to_goal))  # Normalize to [-π, π]\n",
    "        \n",
    "        # Calculate distance to current target\n",
    "        dist_to_target = np.sqrt((x - self.target_x)**2 + (y - self.target_y)**2)\n",
    "        \n",
    "        # Construct state vector with normalized values\n",
    "        state = np.array([\n",
    "            x / self.env_max_x,            # Normalized x position\n",
    "            y / self.env_max_y,            # Normalized y position\n",
    "            np.cos(theta),                 # Orientation (cos)\n",
    "            np.sin(theta),                 # Orientation (sin)\n",
    "            distance / 10.0,               # Distance to goal (normalized)\n",
    "            np.cos(angle_to_goal),         # Angle to goal (cos)\n",
    "            np.sin(angle_to_goal),         # Angle to goal (sin)\n",
    "            dist_to_target / 5.0           # Distance to current target (normalized)\n",
    "        ])\n",
    "        \n",
    "        return state, pose\n",
    "    \n",
    "    def set_target(self, action, pose):\n",
    "        # Scale actions from [-1,1] to position offsets\n",
    "        x, y = pose[0], pose[1]\n",
    "        \n",
    "        # Convert action to target coordinates\n",
    "        self.target_x = x + action[0] * 1.0  # Max movement of 1 unit\n",
    "        self.target_y = y + action[1] * 1.0\n",
    "        \n",
    "        # Clip target coordinates to environment boundaries\n",
    "        self.target_x = np.clip(self.target_x, self.env_min_x, self.env_max_x)\n",
    "        self.target_y = np.clip(self.target_y, self.env_min_y, self.env_max_y)\n",
    "        \n",
    "        return self.target_x, self.target_y\n",
    "    \n",
    "    def step(self, action, duration=0.5):\n",
    "        # Get current state and position\n",
    "        state, pose = self.get_state()\n",
    "        \n",
    "        # Set target position based on action\n",
    "        x_target, y_target = self.set_target(action, pose)\n",
    "        \n",
    "        # Calculate initial distance to goal\n",
    "        distance_to_goal = np.sqrt((pose[0] - self.goal_x)**2 + (pose[1] - self.goal_y)**2)\n",
    "        \n",
    "        # Initialize previous distance if first step\n",
    "        if self.prev_distance is None:\n",
    "            self.prev_distance = distance_to_goal\n",
    "        \n",
    "        # Move robot towards target for 'duration' seconds\n",
    "        self.t.start()\n",
    "        start_time = self.t.get()\n",
    "        \n",
    "        while self.t.get() - start_time < duration:\n",
    "            delta_t = self.t.elapsed()\n",
    "            \n",
    "            # Get current position\n",
    "            pose = self.cart2d.get_pose()\n",
    "            \n",
    "            # Calculate target velocities using polar controller\n",
    "            target_v, target_w = self.polar_position.evaluate(delta_t, x_target, y_target, pose)\n",
    "            \n",
    "            # Apply wheel control\n",
    "            current_left, current_right = self.cart2d.get_wheel_speed()\n",
    "            torque_left, torque_right = wheel_speed_control.evaluate(\n",
    "                delta_t, target_v, target_w, current_left, current_right\n",
    "            )\n",
    "            \n",
    "            # Update robot simulation\n",
    "            self.cart2d.evaluate(delta_t, torque_left, torque_right)\n",
    "            \n",
    "            # Update position in DDS\n",
    "            pose = self.cart2d.get_pose()\n",
    "            self.dds.publish('X', pose[0], DDS.DDS_TYPE_FLOAT)\n",
    "            self.dds.publish('Y', pose[1], DDS.DDS_TYPE_FLOAT)\n",
    "            self.dds.publish('Theta', pose[2], DDS.DDS_TYPE_FLOAT)\n",
    "            \n",
    "            # Small delay for simulation\n",
    "            self.t.sleep(0.005)\n",
    "        \n",
    "        # Get new state after movement\n",
    "        next_state, new_pose = self.get_state()\n",
    "        \n",
    "        # Calculate new distance to goal\n",
    "        new_distance = np.sqrt((new_pose[0] - self.goal_x)**2 + (new_pose[1] - self.goal_y)**2)\n",
    "        \n",
    "        # Calculate reward\n",
    "        reward = 0\n",
    "        done = False\n",
    "        \n",
    "        # Goal reached\n",
    "        if new_distance < 0.2:\n",
    "            reward = 100\n",
    "            done = True\n",
    "        else:\n",
    "            # Reward for getting closer to goal\n",
    "            progress = self.prev_distance - new_distance\n",
    "            reward += progress * 20\n",
    "            \n",
    "            # Time penalty\n",
    "            reward -= 0.1\n",
    "            \n",
    "            # Penalty for moving away from goal\n",
    "            if progress < -0.05:\n",
    "                reward -= 5\n",
    "        \n",
    "        # Update previous distance\n",
    "        self.prev_distance = new_distance\n",
    "        \n",
    "        return next_state, reward, done\n",
    "\n",
    "    def reset(self):\n",
    "        # Initialize a new cart2d object instead of using reset()\n",
    "        global cart2d  # Use the global variable cart2d\n",
    "        \n",
    "        # Recreate cart2d with the same parameters\n",
    "        cart2d = TwoWheelsCart2DEncodersOdometry(1.0,        # 1 kg\n",
    "                                                0.3,        # 30cm radius\n",
    "                                                0.9, 0.8,   # linear and angular friction\n",
    "                                                0.04, 0.04, # traction wheels radius 4cm\n",
    "                                                0.4,        # traction wheels timebase 40cm\n",
    "                                                0.03, 0.03, # encoder wheels radius 3cm\n",
    "                                                0.5,        # encoder wheels timebase 50cm\n",
    "                                                4096)       # resolution 4096 tick/rev\n",
    "        \n",
    "        # Update the reference in the environment\n",
    "        self.cart2d = cart2d\n",
    "        \n",
    "        # Reset previous distance\n",
    "        self.prev_distance = None\n",
    "        \n",
    "        # Small delay for stabilization\n",
    "        time.sleep(0.1)\n",
    "        \n",
    "        # Return initial state\n",
    "        state, _ = self.get_state()\n",
    "        return state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a40497",
   "metadata": {},
   "source": [
    "## Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6359aac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main training function\n",
    "def train_rl_navigation(agent, env, episodes=300, steps_per_episode=50):\n",
    "    rewards_history = []\n",
    "    \n",
    "    for episode in range(episodes):\n",
    "        # Reset environment e ottieni lo stato iniziale\n",
    "        state = env.reset()  # Aggiungi questa riga per inizializzare state\n",
    "        episode_reward = 0\n",
    "        \n",
    "        for step in range(steps_per_episode):\n",
    "            # Select action\n",
    "            action, _ = agent.select_action(state)\n",
    "            \n",
    "            # Execute action\n",
    "            next_state, reward, done = env.step(action)\n",
    "            \n",
    "            # Store experience\n",
    "            agent.remember(state, action, reward, next_state, done)\n",
    "            \n",
    "            # Train agent\n",
    "            agent.train()\n",
    "            \n",
    "            # Update state and reward\n",
    "            state = next_state\n",
    "            episode_reward += reward\n",
    "            \n",
    "            # End episode if done\n",
    "            if done:\n",
    "                print(f\"Episode {episode+1}: Goal reached in {step+1} steps!\")\n",
    "                break\n",
    "        \n",
    "        # Store episode reward\n",
    "        rewards_history.append(episode_reward)\n",
    "        \n",
    "        # Output information\n",
    "        print(f\"Episode {episode+1}/{episodes}, Reward: {episode_reward:.2f}, Steps: {step+1}\")\n",
    "        \n",
    "        # Save model periodically\n",
    "        if (episode + 1) % 50 == 0:\n",
    "            agent.save_models(f\"actor_ep{episode+1}.pth\", f\"critic_ep{episode+1}.pth\")\n",
    "            \n",
    "            # Plot rewards\n",
    "            plt.figure(figsize=(10, 5))\n",
    "            plt.plot(rewards_history)\n",
    "            plt.title('Reward per Episode')\n",
    "            plt.xlabel('Episode')\n",
    "            plt.ylabel('Total Reward')\n",
    "            plt.savefig(f'rewards_ep{episode+1}.png')\n",
    "            plt.close()\n",
    "    \n",
    "    # Save final model\n",
    "    agent.save_models()\n",
    "    return rewards_history\n",
    "\n",
    "# Function to use a pre-trained agent\n",
    "def use_trained_agent(env, agent, episodes=5, steps_per_episode=100):\n",
    "    # Load pre-trained models\n",
    "    agent.load_models()\n",
    "    \n",
    "    for episode in range(episodes):\n",
    "        # Reset e ottieni lo stato iniziale\n",
    "        state = env.reset()  # Aggiungi questa riga\n",
    "        total_reward = 0\n",
    "        \n",
    "        for step in range(steps_per_episode):\n",
    "            # Use deterministic action (no exploration)\n",
    "            _, action_mean = agent.select_action(state)\n",
    "            next_state, reward, done = env.step(action_mean)\n",
    "            \n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "            \n",
    "            if done:\n",
    "                print(f\"Goal reached in {step+1} steps!\")\n",
    "                break\n",
    "        \n",
    "        print(f\"Episode {episode+1}: Total reward {total_reward:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2fc52ce",
   "metadata": {},
   "source": [
    "## Main Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6153c755",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_rl_loop():\n",
    "    # Initialize data plotters for visualization\n",
    "    left_w = DataPlotter()\n",
    "    left_w.set_x(\"time (seconds)\")\n",
    "    left_w.add_y(\"target_speed\", \"Target Left Speed\")\n",
    "    left_w.add_y(\"current_speed\", \"Current Left Speed\")\n",
    "\n",
    "    right_w = DataPlotter()\n",
    "    right_w.set_x(\"time (seconds)\")\n",
    "    right_w.add_y(\"target_speed\", \"Target Right Speed\")\n",
    "    right_w.add_y(\"current_speed\", \"Current Right Speed\")\n",
    "    \n",
    "    # Initialize reward plotter\n",
    "    reward_plotter = DataPlotter()\n",
    "    reward_plotter.set_x(\"time (seconds)\")\n",
    "    reward_plotter.add_y(\"reward\", \"Cumulative Reward\")\n",
    "    \n",
    "    # Initialize RL components\n",
    "    state_size = 8  # State dimension\n",
    "    action_size = 2  # (x,y) target\n",
    "    rl_agent = A2CAgent(state_size, action_size)\n",
    "    \n",
    "    # Initialize RL environment\n",
    "    rl_env = RobotEnvironment(dds, cart2d, polar_position)\n",
    "    \n",
    "    # Performance tracking\n",
    "    episode = 0\n",
    "    total_reward = 0\n",
    "    \n",
    "    # Initialize simulation\n",
    "    t = Time()\n",
    "    t.start()\n",
    "    pose = cart2d.get_pose()\n",
    "    \n",
    "    # Get initial state\n",
    "    state = rl_env.get_state()[0]\n",
    "    \n",
    "    # Main loop\n",
    "    try:\n",
    "        while t.get() < 60:  # Extended time window for learning\n",
    "            # RL agent selects action (new target position)\n",
    "            action, _ = rl_agent.select_action(state)\n",
    "            \n",
    "            # Update target x,y based on agent's action\n",
    "            x_target, y_target = rl_env.set_target(action, pose)\n",
    "            \n",
    "            # Inner loop for low-level robot control\n",
    "            step_start_time = t.get()\n",
    "            step_duration = 0.3  # Duration of each RL step\n",
    "            \n",
    "            while t.get() - step_start_time < step_duration:\n",
    "                t.sleep(0.005)\n",
    "                delta_t = t.elapsed()\n",
    "                \n",
    "                # Get current position\n",
    "                pose = cart2d.get_pose()\n",
    "                \n",
    "                # Calculate target velocities using polar controller\n",
    "                target_v, target_w = polar_position.evaluate(delta_t, x_target, y_target, pose)\n",
    "                \n",
    "                # Apply wheel control\n",
    "                current_left, current_right = cart2d.get_wheel_speed()\n",
    "                torque_left, torque_right = wheel_speed_control.evaluate(delta_t,\n",
    "                                                                       target_v, target_w,\n",
    "                                                                       current_left, current_right)\n",
    "                \n",
    "                # Update robot simulation\n",
    "                cart2d.evaluate(delta_t, torque_left, torque_right)\n",
    "                \n",
    "                # Get new position\n",
    "                pose = cart2d.get_pose()\n",
    "                \n",
    "                # Publish to DDS\n",
    "                dds.publish('X', pose[0], DDS.DDS_TYPE_FLOAT)\n",
    "                dds.publish('Y', pose[1], DDS.DDS_TYPE_FLOAT)\n",
    "                dds.publish('Theta', pose[2], DDS.DDS_TYPE_FLOAT)\n",
    "                \n",
    "                # Update plots\n",
    "                left_w.append_x(t.get())\n",
    "                left_w.append_y(\"current_speed\", current_left)\n",
    "                left_w.append_y(\"target_speed\", wheel_speed_control.target_left)\n",
    "                \n",
    "                right_w.append_x(t.get())\n",
    "                right_w.append_y(\"current_speed\", current_right)\n",
    "                right_w.append_y(\"target_speed\", wheel_speed_control.target_right)\n",
    "            \n",
    "            # Get new state and calculate reward\n",
    "            next_state, reward, done = rl_env.step(action, 0)  # 0 duration because movement already happened\n",
    "            \n",
    "            # Store experience for learning\n",
    "            rl_agent.remember(state, action, reward, next_state, done)\n",
    "            \n",
    "            # Train agent\n",
    "            rl_agent.train()\n",
    "            \n",
    "            # Update state\n",
    "            state = next_state\n",
    "            \n",
    "            # Accumulate reward\n",
    "            total_reward += reward\n",
    "            \n",
    "            # Update reward plot\n",
    "            reward_plotter.append_x(t.get())\n",
    "            reward_plotter.append_y(\"reward\", total_reward)\n",
    "            \n",
    "            # Check if episode ended\n",
    "            if done or t.get() >= 60:\n",
    "                # Print episode information\n",
    "                print(f\"Episode {episode} completed. Total reward: {total_reward}\")\n",
    "                \n",
    "                # Periodically save model\n",
    "                if episode % 10 == 0:\n",
    "                    rl_agent.save_models(f\"actor_ep{episode}.pth\", f\"critic_ep{episode}.pth\")\n",
    "                \n",
    "                # Reset for next episode\n",
    "                total_reward = 0\n",
    "                episode += 1\n",
    "    \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Manual interruption. Saving model and data...\")\n",
    "        rl_agent.save_models()\n",
    "        \n",
    "    finally:\n",
    "        # Clean up DDS connection\n",
    "        dds.stop()\n",
    "        \n",
    "        # Display plots\n",
    "        left_w.plot()\n",
    "        right_w.plot()\n",
    "        reward_plotter.plot()\n",
    "        \n",
    "        # Save final model\n",
    "        rl_agent.save_models(\"actor_final.pth\", \"critic_final.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "334669a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/s6/rbq2n_1j59ncwkjkh13j8rlc0000gn/T/ipykernel_30079/2433870158.py:93: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1729647038473/work/torch/csrc/utils/tensor_new.cpp:281.)\n",
      "  states = torch.FloatTensor([experience[0] for experience in minibatch]).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1/300, Reward: -217.70, Steps: 50\n",
      "Episode 2/300, Reward: -181.98, Steps: 50\n",
      "Episode 3/300, Reward: -164.99, Steps: 50\n",
      "Episode 4/300, Reward: -195.20, Steps: 50\n",
      "Episode 5/300, Reward: -196.15, Steps: 50\n",
      "Episode 6/300, Reward: -156.32, Steps: 50\n",
      "Episode 7/300, Reward: -160.11, Steps: 50\n",
      "Episode 8/300, Reward: -210.74, Steps: 50\n",
      "Episode 9/300, Reward: -172.60, Steps: 50\n",
      "Episode 10/300, Reward: -189.07, Steps: 50\n",
      "Episode 11/300, Reward: -206.15, Steps: 50\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "float division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m env \u001b[38;5;241m=\u001b[39m RobotEnvironment(dds, cart2d, polar_position)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Train agent\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m rewards \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_rl_navigation\u001b[49m\u001b[43m(\u001b[49m\u001b[43magent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepisodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m300\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Plot final rewards\u001b[39;00m\n\u001b[1;32m     18\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m5\u001b[39m))\n",
      "Cell \u001b[0;32mIn[13], line 15\u001b[0m, in \u001b[0;36mtrain_rl_navigation\u001b[0;34m(agent, env, episodes, steps_per_episode)\u001b[0m\n\u001b[1;32m     12\u001b[0m action, _ \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mselect_action(state)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Execute action\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m next_state, reward, done \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Store experience\u001b[39;00m\n\u001b[1;32m     18\u001b[0m agent\u001b[38;5;241m.\u001b[39mremember(state, action, reward, next_state, done)\n",
      "Cell \u001b[0;32mIn[12], line 252\u001b[0m, in \u001b[0;36mRobotEnvironment.step\u001b[0;34m(self, action, duration)\u001b[0m\n\u001b[1;32m    249\u001b[0m pose \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcart2d\u001b[38;5;241m.\u001b[39mget_pose()\n\u001b[1;32m    251\u001b[0m \u001b[38;5;66;03m# Calculate target velocities using polar controller\u001b[39;00m\n\u001b[0;32m--> 252\u001b[0m target_v, target_w \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolar_position\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdelta_t\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_target\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_target\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpose\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[38;5;66;03m# Apply wheel control\u001b[39;00m\n\u001b[1;32m    255\u001b[0m current_left, current_right \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcart2d\u001b[38;5;241m.\u001b[39mget_wheel_speed()\n",
      "File \u001b[0;32m~/Desktop/Year 24-25 University-DMI/Robotic-Systems/alvik_pathfinding/lib/system/polar.py:30\u001b[0m, in \u001b[0;36mPolar2DController.evaluate\u001b[0;34m(self, delta_t, xt, yt, current_pose)\u001b[0m\n\u001b[1;32m     27\u001b[0m     distance \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39mdistance\n\u001b[1;32m     28\u001b[0m     heading_error \u001b[38;5;241m=\u001b[39m normalize_angle(heading_error \u001b[38;5;241m+\u001b[39m math\u001b[38;5;241m.\u001b[39mpi)\n\u001b[0;32m---> 30\u001b[0m v_target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdelta_t\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdistance\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m w_target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mangular\u001b[38;5;241m.\u001b[39mevaluate(delta_t, heading_error)\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (v_target, w_target)\n",
      "File \u001b[0;32m~/Desktop/Year 24-25 University-DMI/Robotic-Systems/alvik_pathfinding/lib/system/controllers.py:60\u001b[0m, in \u001b[0;36mPID_Controller.evaluate\u001b[0;34m(self, delta_t, _error)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mevaluate\u001b[39m(\u001b[38;5;28mself\u001b[39m, delta_t: \u001b[38;5;28mfloat\u001b[39m, _error: \u001b[38;5;28mfloat\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mfloat\u001b[39m:\n\u001b[1;32m     59\u001b[0m     out_PI \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mevaluate(delta_t, _error)\n\u001b[0;32m---> 60\u001b[0m     out_D \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mD\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdelta_t\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_error\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out_PI \u001b[38;5;241m+\u001b[39m out_D \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkd\n",
      "File \u001b[0;32m~/Desktop/Year 24-25 University-DMI/Robotic-Systems/alvik_pathfinding/lib/system/basic.py:20\u001b[0m, in \u001b[0;36mDerivator.evaluate\u001b[0;34m(self, delta_t, _input)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mevaluate\u001b[39m(\u001b[38;5;28mself\u001b[39m, delta_t, _input):\n\u001b[0;32m---> 20\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43m(\u001b[49m\u001b[43m_input\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprev_input\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdelta_t\u001b[49m\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprev_input \u001b[38;5;241m=\u001b[39m _input\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: float division by zero"
     ]
    }
   ],
   "source": [
    "# Choose execution mode\n",
    "mode = \"main\"  # options: \"main\", \"train\", \"use_trained\"\n",
    "mode = \"train\"\n",
    "\n",
    "if mode == \"main\":\n",
    "    main_rl_loop()\n",
    "elif mode == \"train\":\n",
    "    # Initialize agent and environment\n",
    "    state_size = 8\n",
    "    action_size = 2\n",
    "    agent = A2CAgent(state_size, action_size)\n",
    "    env = RobotEnvironment(dds, cart2d, polar_position)\n",
    "    \n",
    "    # Train agent\n",
    "    rewards = train_rl_navigation(agent, env, episodes=300)\n",
    "    \n",
    "    # Plot final rewards\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(rewards)\n",
    "    plt.title('Reward per Episode')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Total Reward')\n",
    "    plt.savefig('final_rewards.png')\n",
    "    plt.show()\n",
    "elif mode == \"use_trained\":\n",
    "    # Initialize agent and environment\n",
    "    state_size = 8\n",
    "    action_size = 2\n",
    "    agent = A2CAgent(state_size, action_size)\n",
    "    env = RobotEnvironment(dds, cart2d, polar_position)\n",
    "    \n",
    "    # Use trained agent\n",
    "    use_trained_agent(env, agent, episodes=5)\n",
    "\n",
    "# Cleanup resources when done\n",
    "dds.stop()\n",
    "print(\"DDS connection closed\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "robotic_systems",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
